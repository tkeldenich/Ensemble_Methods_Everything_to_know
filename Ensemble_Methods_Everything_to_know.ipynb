{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP5brGMArT7YBZQjDuBP3NC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tkeldenich/Ensemble_Methods_Everything_to_know/blob/main/Ensemble_Methods_Everything_to_know.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ensemble Methods – Everything you need to know now**"
      ],
      "metadata": {
        "id": "yAtAYCGq1UK5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this tutorial, we will see how to use Ensemble methods, a technique to improve your Deep Learning models precision.\n",
        "\n",
        "The concept of Ensemble is simple: **gather predictions of several Machine Learning Algorithms to obtain an optimal result.**\n",
        "\n",
        "For example, by averaging a Decision Tree and a Linear Regression to get a new result.\n",
        "\n",
        "The idea is to take several models that each have their qualities and flaws. We use them **together (ensemble)** to balance their biases and get a better prediction.\n",
        "\n",
        "**Imagine you have to assemble some Ikea furniture.**\n",
        "\n",
        "You know how to drive nails into the wood.\n",
        "\n",
        "But screws are not your thing.\n",
        "\n",
        "You’ll go much faster if you call your friend, screwdriver expert.\n",
        "\n",
        "**Here, it’s the same thing.**\n",
        "\n",
        "> Every Machine Learning model has biases.\n",
        "\n",
        "If you use a new algorithm and combine it with the old one, you can correct these biases and get a better result.\n",
        "\n",
        "**And it works!**\n",
        "\n",
        "Nowadays most of the Machine Learning competitions winners use Ensemble to create the best algorithms.\n",
        "\n",
        "In this article, I propose you to see the main methods **in theory AND in practice** using the Scikit-Learn library."
      ],
      "metadata": {
        "id": "VPNLGeXI1u6l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Our dataset**"
      ],
      "metadata": {
        "id": "1vbWY00X9cz-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’ll use the same dataset as in [our article to learn Machine Learning with sklearn.](https://inside-machinelearning.com/en/scikit-learn-project-start-ml/)\n",
        "\n",
        "No worries if you didn’t read the article.\n",
        "\n",
        "**You can still follow this tutorial without any problem 😉**\n",
        "\n",
        "First of all, load my Git folder which lists the interesting datasets of ML:"
      ],
      "metadata": {
        "id": "r4dSMYPx2AND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/tkeldenich/datasets.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAYXZt77mHaQ",
        "outputId": "568dd626-b3ab-400e-9336-50622b640a6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'datasets'...\n",
            "remote: Enumerating objects: 30, done.\u001b[K\n",
            "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 30 (delta 8), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (30/30), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We choose the *winequality-white.csv* dataset and import it into a Pandas DataFrame :\n"
      ],
      "metadata": {
        "id": "cgg2iQgI2KNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/datasets/winequality-white.csv\", sep=\";\")\n",
        "df.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "Yy4uTGacl8iR",
        "outputId": "f8ed0cdf-42a0-4e1e-b05e-258445693af9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
              "0            7.0              0.27         0.36            20.7      0.045   \n",
              "1            6.3              0.30         0.34             1.6      0.049   \n",
              "2            8.1              0.28         0.40             6.9      0.050   \n",
              "\n",
              "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
              "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
              "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
              "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
              "\n",
              "   alcohol  quality  \n",
              "0      8.8        6  \n",
              "1      9.5        6  \n",
              "2     10.1        6  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-deb7277e-0119-4bb0-87a4-8ece454833fc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fixed acidity</th>\n",
              "      <th>volatile acidity</th>\n",
              "      <th>citric acid</th>\n",
              "      <th>residual sugar</th>\n",
              "      <th>chlorides</th>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <th>density</th>\n",
              "      <th>pH</th>\n",
              "      <th>sulphates</th>\n",
              "      <th>alcohol</th>\n",
              "      <th>quality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7.0</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.36</td>\n",
              "      <td>20.7</td>\n",
              "      <td>0.045</td>\n",
              "      <td>45.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>1.0010</td>\n",
              "      <td>3.00</td>\n",
              "      <td>0.45</td>\n",
              "      <td>8.8</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6.3</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.34</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.049</td>\n",
              "      <td>14.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>0.9940</td>\n",
              "      <td>3.30</td>\n",
              "      <td>0.49</td>\n",
              "      <td>9.5</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8.1</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.40</td>\n",
              "      <td>6.9</td>\n",
              "      <td>0.050</td>\n",
              "      <td>30.0</td>\n",
              "      <td>97.0</td>\n",
              "      <td>0.9951</td>\n",
              "      <td>3.26</td>\n",
              "      <td>0.44</td>\n",
              "      <td>10.1</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-deb7277e-0119-4bb0-87a4-8ece454833fc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-deb7277e-0119-4bb0-87a4-8ece454833fc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-deb7277e-0119-4bb0-87a4-8ece454833fc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here a row represents a wine.\n",
        "\n",
        "Each column is a feature of this wine.\n",
        "\n",
        "**The goal is to predict the wine quality by using its features.**\n",
        "\n",
        "If you want a detailed Data Analysis, I refer you to [this article.](https://inside-machinelearning.com/en/scikit-learn-project-start-ml/)\n",
        "\n",
        "**It is a classification problem with 7 classes** (the wine quality ranging from 3 to 9).\n",
        "\n",
        "We separate our dataset between the features X and the label Y:"
      ],
      "metadata": {
        "id": "qjcDTY2R2ME5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_features = df.drop(columns='quality')\n",
        "df_label = df['quality']"
      ],
      "metadata": {
        "id": "QkgaCURspa38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then between the training data and the test data:"
      ],
      "metadata": {
        "id": "LA0m_8n82UdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_features, df_label, test_size=0.20)"
      ],
      "metadata": {
        "id": "0AC7m2FapmGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Done! We are ready to use Ensemble’s methods.**\n",
        "\n",
        "Our goal is to do better than [previous Machine Learning algo’s!](https://inside-machinelearning.com/en/scikit-learn-project-start-ml/)\n",
        "\n",
        "Most of them hardly reach an accuracy of 45%.\n",
        "\n",
        "**But the Decision Tree outperforms them all, with an accuracy of 60%.**\n",
        "\n",
        "Let’s try to do better with Ensembles!"
      ],
      "metadata": {
        "id": "l_JfyvoP2Wo5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Bagging Algorithms**"
      ],
      "metadata": {
        "id": "Jx1xpDNWb9Ii"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bagging algorithms** use several Machine Learning algorithms called **Weak Learners.**\n",
        "\n",
        "They are trained independently on the dataset. **This is called parallel training.**\n",
        "\n",
        "**Once trained, we gather their results into final predictions.**\n",
        "\n",
        "In order to do this we use, most often, the calculation of **the average of the predictions.**\n",
        "\n",
        "Let’s see the different Bagging algorithms that sklearn offers.\n",
        "\n"
      ],
      "metadata": {
        "id": "gKKEWJBO2erp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Bagging Classifier**"
      ],
      "metadata": {
        "id": "XFBhqaOD2N4g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Theory**"
      ],
      "metadata": {
        "id": "_ub8EZxJ2mP_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we have the Bagging Classifier. It is an ensemble of **similar algorithms.**\n",
        "\n",
        "Each are trained on a sub-dataset.\n",
        "\n",
        "The idea is to randomly separate the dataset into several sub-datasets, for example 3.\n",
        "\n",
        "Each of these 3 sub-datasets will enable to train an algorithm.\n",
        "\n",
        "At the end, we will have 3 algorithms, trained on different datasets.\n",
        "\n",
        "**I insist on the fact that these algorithms are similar.**\n",
        "\n",
        "That is to say that if you choose a Decision Tree, the 3 algorithms will be Decision Trees with the same hyperparameters baseline.\n",
        "\n",
        "**The only difference is the data they are trained with.**\n",
        "\n",
        "To use this set on new X_test data, we run the 3 algorithms.\n",
        "\n",
        "We will have 3 results:\n",
        "\n",
        "- y_pred_1\n",
        "- y_pred_2\n",
        "- y_pred_3\n",
        "\n",
        "The final result is obtained by “aggregating” these results, either by calculating the average of the 3, or by taking the majority choice (this is called “a vote”).\n",
        "\n",
        "**Here is the scheme of the Bagging algorithm:**\n",
        "\n",
        "<image src=\"https://inside-machinelearning.com/wp-content/uploads/2022/12/bagging-classifier-2048x991.jpeg\" width=\"800\">"
      ],
      "metadata": {
        "id": "fr9RY3v82mcf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Understanding the vote**"
      ],
      "metadata": {
        "id": "LsZ4rdqO3Brj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s take the first wine. Imagine that we have **the following probabilistic results** for the 3 algorithms:\n",
        "\n",
        "1.   Éprobability of being of quality 3 = **30%** – probability of being of quality 4 = **70%**\n",
        "2.   Éprobability of being of quality 3 = **80%** – probability of being of quality 4 = **20%**\n",
        "3.   Éprobability of being of quality 3 = **70%** – probability of being quality 4 = **30%**\n",
        "\n",
        "The average gives us :\n",
        "\n",
        "- probability of being of quality 3 = (30 + 80 + 70) / 3 = **60%**\n",
        "\n",
        "- probability of being of quality 4 = (70 + 20 + 30) / 3 = **40%**\n",
        "\n",
        "The Ensemble predicts that the wine is of quality 3!"
      ],
      "metadata": {
        "id": "ADVs7lNf3BzZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Understanding the vote**"
      ],
      "metadata": {
        "id": "OEHah1QO3B42"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s take the first wine again. But this time we will NOT take **the probabilistic results.**\n",
        "\n",
        "We take the direct results of the algorithms. That is, we take the highest probability and assign it a 1. We assign a 0 to the other probabilities.\n",
        "\n",
        "Here is what it gives us for the 3 algorithms:\n",
        "\n",
        "1.   result for quality 3 = **0** – result for quality 4 = **1**\n",
        "2.   result for quality 3 = **1** – result for quality 4 = **0**\n",
        "3.   result for quality 3 = **1** – result for quality 4 = **0**\n",
        "\n",
        "Here we look at the quality of wine that has the most 1’s.\n",
        "\n",
        "Quality 3 wins!\n",
        "\n",
        "The Ensemble, here also, predicts that the wine is of quality 3.\n",
        "\n"
      ],
      "metadata": {
        "id": "pBpnORg23CE3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Practice**"
      ],
      "metadata": {
        "id": "lG6LSaSb3mrN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Bagging classifier can seem complicated.\n",
        "\n",
        "**Sklearn makes it super simple!**\n",
        "\n",
        "Here is the code to initialize a Bagging Classifier and train it on our data:"
      ],
      "metadata": {
        "id": "LLN7iG703mta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "baggingClassifier = BaggingClassifier(n_estimators=100)\n",
        "baggingClassifier.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhN2OZmv2Fcm",
        "outputId": "33a7355a-c70b-478b-f657-f3fe18424131"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BaggingClassifier(n_estimators=100)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see that we don’t need to separate our dataset.\n",
        "\n",
        "**Sklearn takes care of everything.**\n",
        "\n",
        "We can indicate the model on which the Ensemble should be based. But by default, if we don’t indicate anything, sklearn will take a Decision Tree.\n",
        "\n",
        "We display the result:"
      ],
      "metadata": {
        "id": "XrVFYAYQ20XW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "baggingClassifier.score(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCJsE-5c2Jxu",
        "outputId": "be585773-3db2-4375-b60f-be8bedf4ec3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6795918367346939"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We obtain an accuracy of 67.9% ! This is 7.9% more than a classical Decision Tree.**\n",
        "\n",
        "This is a considerable improvement!"
      ],
      "metadata": {
        "id": "jb8RrHGH3uLZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Voting Classifier**"
      ],
      "metadata": {
        "id": "bBJp-tvI2yzm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Theory**"
      ],
      "metadata": {
        "id": "8F15u8ER3yVX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have the Voting Classifier.\n",
        "\n",
        "It is very similar to the Bagging Classifier.\n",
        "\n",
        "But it is different.\n",
        "\n",
        "**Here, the Ensemble is not necessarily composed of similar algorithms.**\n",
        "\n",
        "We can have a Decision Tree and two Linear Regression.\n",
        "\n",
        "**Then, the dataset isn’t separated in a random way.**\n",
        "\n",
        "We keep the complete dataset and we train all the models with it.\n",
        "\n",
        "Finally, we make a vote between algorithms, as we have seen before.\n",
        "\n",
        "Here is the diagram of the Voting Classifier:\n",
        "\n",
        "<image src=\"https://inside-machinelearning.com/wp-content/uploads/2022/12/voting-classifier-2048x1148.jpeg\" width=\"800\">\n",
        "\n",
        "**The Voting Classifier is the lengthiest Bagging Ensemble to set up.**\n",
        "\n",
        "We have to choose each model of our ensemble.\n",
        "\n",
        "We can also add weights to control the importance of each model.\n",
        "\n",
        "Here I choose to assign the most important weight to the Decision Tree, [the best model to solve our problem](https://inside-machinelearning.com/en/scikit-learn-project-start-ml/):"
      ],
      "metadata": {
        "id": "a4nI7apb4Pt8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn import tree\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "decisionTree = tree.DecisionTreeClassifier()\n",
        "logisticRegression = LogisticRegression()\n",
        "SGD = SGDClassifier()\n",
        "GNB = GaussianNB()\n",
        "\n",
        "votingClassifier = VotingClassifier(estimators=[\n",
        "    ('tree', decisionTree), ('lr',logisticRegression), ('sgd', SGD), ('gnb', GNB)],\n",
        "    weights=[2,1,1,0.5])\n",
        "votingClassifier.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4cd7670-8c7c-4a74-a2a7-10b899c373eb",
        "id": "3KMVs5dl2yzq"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VotingClassifier(estimators=[('tree', DecisionTreeClassifier()),\n",
              "                             ('lr', LogisticRegression()),\n",
              "                             ('sgd', SGDClassifier()), ('gnb', GaussianNB())],\n",
              "                 weights=[2, 1, 1, 0.5])"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we compute the Ensemble score:"
      ],
      "metadata": {
        "id": "MsmOwIEU4Fqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "votingClassifier.score(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03c63174-9278-4951-e80a-1ea8b87ee767",
        "id": "2Gkq8KJi2yzr"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6030612244897959"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "60% accuracy. The Voting Classifier did not give the best result.\n",
        "\n",
        "Let’s go on to the next step !\n",
        "\n"
      ],
      "metadata": {
        "id": "42dONRTA4HOC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Random Forest**"
      ],
      "metadata": {
        "id": "T3edokbZ6My1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We arrive at the famous Random Forest.\n",
        "\n",
        "Here, we are expected to break the record!\n",
        "\n",
        "**The Random Forest is a Decision Tree Ensemble** (the best model for our project).\n",
        "\n",
        "As for the Bagging Classifier, we randomly separate our dataset into sub-datasets.\n",
        "\n",
        "**Then we train different Decision Trees on each sub-dataset.**\n",
        "\n",
        "Finally we use the average to obtain the final predictions:\n",
        "\n",
        "<image src=\"https://inside-machinelearning.com/wp-content/uploads/2022/12/random-forest-classifier-2048x962.jpeg\" width=\"800\">\n",
        "\n",
        "The implementation is very simple with sklearn :"
      ],
      "metadata": {
        "id": "upAagS644Lo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "randomForest = RandomForestClassifier(n_estimators=100)\n",
        "randomForest.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0Pmdmef6rD8",
        "outputId": "80416258-65a3-4a5c-eaac-8f7de5336dca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier()"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we compute the score :"
      ],
      "metadata": {
        "id": "MkBGhHlv4bBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "randomForest.score(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iO0k6mZ-9pAf",
        "outputId": "fa73d464-6721-45b7-dc7a-8012b9404ebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**70%, our best score!** 🔥\n",
        "\n",
        "We managed to get a good score even though we started with very little advantage on our side.\n",
        "\n",
        "The first trained model had an accuracy of 46%.\n",
        "\n",
        "**With motivation and the right techniques, we managed to increase this accuracy by more than 24%. That’s huge!**\n",
        "\n",
        "**If you’ve been following the project from the beginning until now, you can congratulate yourself!**\n",
        "\n",
        "Let’s continue with the rest of the Ensemble algorithms.\n",
        "\n"
      ],
      "metadata": {
        "id": "XCJj4L8G4eEv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Boosting Algorithms**"
      ],
      "metadata": {
        "id": "Xp8LWos-b30m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Boosting algorithms** bring together several versions of the same Machine Learning algorithm (Weak Learners).\n",
        "\n",
        "The first version of the model is trained on the dataset.\n",
        "\n",
        "Then we create a second version of the model and we also train it on the dataset.\n",
        "\n",
        "**The goal here is to improve the new model version by targeting the weaknesses of the first one.**\n",
        "\n",
        "Each trained algorithm is a new version of the previous one. **This is called sequential training.**\n",
        "\n",
        "Finally, as for the Bagging algorithms, we gather the results into **final predictions** by calculating **the average of the predictions.**\n",
        "\n",
        "Let’s go on to the algorithms proposed by sklearn !"
      ],
      "metadata": {
        "id": "B2KNpHkj4kBC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **AdaBoost**"
      ],
      "metadata": {
        "id": "x1SD9xjQ-SIU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AdaBoost is short for Adaptive Boosting.**\n",
        "\n",
        "The idea is very similar to the Bagging Classifier concept.\n",
        "\n",
        "**We will create several versions of the model.**\n",
        "\n",
        "However, here we do not train it on different sub-datasets.\n",
        "\n",
        "We train the first model on the whole dataset.\n",
        "\n",
        "Then we observe its predictions.\n",
        "\n",
        "**We create a new version of the model by adjusting the weights at the level of the incorrect predictions of the first one.**\n",
        "\n",
        "We repeat the process until we are satisfied.\n",
        "\n",
        "In this way the new model versions focus more on the difficult cases identified by the previous ones.\n",
        "\n",
        "**Thus each model balances the flaws of the others.**\n",
        "\n",
        "The final prediction is a vote or an average of all the models.\n",
        "\n",
        "Here is the Boosting Classifier scheme:\n",
        "\n",
        "<image src=\"https://inside-machinelearning.com/wp-content/uploads/2022/12/boosting-classifier-1-2048x1265.jpeg\" width=\"800\">\n",
        "\n",
        "We use AdaBoost with sklearn like so:"
      ],
      "metadata": {
        "id": "iyzy07sI4wJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "adaBoost = AdaBoostClassifier(n_estimators=100)\n",
        "adaBoost.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hE3OOWrD-I9I",
        "outputId": "36ad5e1e-b62e-45b9-c5d9-51256e742dc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AdaBoostClassifier(n_estimators=100)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We compute the score :"
      ],
      "metadata": {
        "id": "3lcaJ-FP49vg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "adaBoost.score(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOcDjEsw-X23",
        "outputId": "a8e4036f-2a3c-4465-91b7-4626b6db799f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.44387755102040816"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We get an accuracy of 44.3%.\n",
        "\n",
        "For the moment AdaBoost gives us the worst score among the Ensemble methods.\n",
        "\n",
        "**Actually, AdaBoost is THE first Boosting algorithm created.**\n",
        "\n",
        "Today, new Boosting algorithms exist, notably available via the LightGBM, XGBoost or CatBoost libraries."
      ],
      "metadata": {
        "id": "s7_IZZTw4_fq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Gradient Boosting**"
      ],
      "metadata": {
        "id": "7JUaMi0Y-r9C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Historically, AdaBoost is the first boosting algorithm.\n",
        "\n",
        "Gradient Boosting follows it closely.\n",
        "\n",
        "**As a matter of fact AdaBoost allows to solve specific problems.**\n",
        "\n",
        "Seeing its promising results, researchers decided to generalize the mathematical approach of AdaBoost with the Gradient Boosting algorithm.\n",
        "\n",
        "Thus Gradient Boosting can solve many more problems than AdaBoost.\n",
        "\n",
        "And even if AdaBoost is the oldest, it is now seen as a special case of Gradient Boosting.\n",
        "\n",
        "I don’t want to dive into the mathematical details of the algorithm.\n",
        "\n",
        "**Just keep in mind that GradBoost is a generalization of AdaBoost.**\n",
        "\n",
        "**That is, it is efficient on most datasets, unlike AdaBoost which is more specific.**\n",
        "\n",
        "And if you are a curious mathematician, here are the resources that will interest you:\n",
        "\n",
        "- Invent Adaboost, the first successful boosting algorithm [Freund et al., 1996, Freund and Schapire, 1997]\n",
        "- Formulate Adaboost as gradient descent with a special loss function[Breiman et al., 1998, Breiman, 1999]\n",
        "- Generalize Adaboost to Gradient Boosting in order to handle a variety of loss functions [Friedman et al., 2000, Friedman, 2001]\n",
        "\n",
        "The scheme of Gradient Boosting is the same as AdaBoost since only the underlying mathematics is different.\n",
        "\n",
        "Here is how to use Gradient Boosting with sklearn:"
      ],
      "metadata": {
        "id": "H6Li2rCS5FX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "gradientBoosting = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
        "                                              max_depth=1, random_state=0)\n",
        "gradientBoosting.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHzE-uXJ-ZCr",
        "outputId": "40f9f43d-7906-425a-81f8-7abbfa562ab0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradientBoostingClassifier(learning_rate=1.0, max_depth=1, random_state=0)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We compute the score:"
      ],
      "metadata": {
        "id": "soEYrG305Poi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gradientBoosting.score(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9soeOC9R-l-z",
        "outputId": "e02b03de-5f4c-4bb0-bc75-4f61cff2dea9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5244897959183673"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "52% accuracy, the improvement over AdaBoost is **noticeable.**\n",
        "\n",
        "But let’s see an even more recent evolution of Gradient Boosting."
      ],
      "metadata": {
        "id": "hccn-LTC5RJT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Histogram-Based Gradient Boosting**"
      ],
      "metadata": {
        "id": "M10Khefd2eqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To put it simply, Histogram-Based Gradient Boosting is a faster version of GradBoost.**\n",
        "\n",
        "Faster and therefore easier to calculate.\n",
        "\n",
        "It allows to allocate more time to the optimization. And thus to obtain better results.\n",
        "\n",
        "Histogram-Based Gradient Boosting uses a data structure called histogram in which the elements are implicitly ordered.\n",
        "\n",
        "**This enables a fast processing of the information.**\n",
        "\n",
        "Again, you can refer to the AdaBoost scheme as the only difference is mathematical.\n",
        "\n",
        "Here is how to use Histogram-Based Gradient Boosting:"
      ],
      "metadata": {
        "id": "QuA9eiCa5Tno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "\n",
        "histGradientBoostingClassifier = HistGradientBoostingClassifier(max_iter=100)\n",
        "histGradientBoostingClassifier.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56fa20fa-ee07-4cc0-e6bc-879e782c5ff8",
        "id": "yoIjXmfA2eqK"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HistGradientBoostingClassifier()"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we compute the score :"
      ],
      "metadata": {
        "id": "GO37hm9h5Y8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "histGradientBoostingClassifier.score(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4abb4179-27e4-4920-bab6-5ad1ab62cafd",
        "id": "ge2CNwcb2eqL"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6714285714285714"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "67.1% accuracy. Congratulations!\n",
        "\n",
        "**That is the best score for Boosting algorithms!**\n",
        "\n",
        "We can finally move on to the last types of Ensemble algorithms…"
      ],
      "metadata": {
        "id": "P8AvFbSh5apK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Stacking Algorithms**"
      ],
      "metadata": {
        "id": "OK_w0ML8nqQj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stacking algorithms** use the results of several Machine Learning algorithms. These results are then used as input for a final algorithm that gives the prediction.\n",
        "\n",
        "The algorithms are trained independently on the dataset.\n",
        "\n",
        "**As with Bagging, it is a parallel training.**\n",
        "\n",
        "The predictions are then gathered (stacked).\n",
        "\n",
        "A last algorithm is finally used on all the results to obtain **the final predictions.**\n",
        "\n",
        "Sklearn offers us a single Stacking algorithm:"
      ],
      "metadata": {
        "id": "VNyjF2Ne5dvV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Stacking Classifier**"
      ],
      "metadata": {
        "id": "BzytIAoL4cHD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Stacking Classifier works according to the classical Stacking method.**\n",
        "\n",
        "On top of that, the final model is trained using cross-validation, a technique we’ve already discussed in [this article.](https://inside-machinelearning.com/en/cross-validation-tutorial/)\n",
        "\n",
        "Here is the scheme of the Stacking Classifier:\n",
        "\n",
        "<image src=\"https://inside-machinelearning.com/wp-content/uploads/2022/12/stacking-classifier-2048x1110.jpeg\" width=\"800\">\n",
        "\n",
        "To use it, we specify the models we want to use:\n",
        "\n",
        "the models to be trained in parallel\n",
        "the final prediction model"
      ],
      "metadata": {
        "id": "APnN6-6S5jZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn import tree\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "decisionTree = tree.DecisionTreeClassifier()\n",
        "logisticRegression = LogisticRegression()\n",
        "SGD = SGDClassifier()\n",
        "GNB = GaussianNB()\n",
        "finalClassifier = LogisticRegression()\n",
        "\n",
        "stackingClassifier = StackingClassifier(estimators=[\n",
        "    ('tree', decisionTree), ('lr',logisticRegression), ('sgd', SGD), ('gnb', GNB)],\n",
        "    final_estimator=finalClassifier)\n",
        "stackingClassifier.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "086e514d-9a3e-4c2e-a187-efa72e64fb00",
        "id": "jtrctxpT4cHG"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StackingClassifier(estimators=[('tree', DecisionTreeClassifier()),\n",
              "                               ('lr', LogisticRegression()),\n",
              "                               ('sgd', SGDClassifier()),\n",
              "                               ('gnb', GaussianNB())],\n",
              "                   final_estimator=LogisticRegression())"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To use it, we specify the models we want to use:**\n",
        "\n",
        "- the models to be trained in parallel\n",
        "- the final prediction model"
      ],
      "metadata": {
        "id": "hDEtP9kB5qy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stackingClassifier.score(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fec192c-bf42-4e8d-f105-1d81f860eaee",
        "id": "d1MJjffI4cHH"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4775510204081633"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this last method, we obtain an accuracy of 47.7% !"
      ],
      "metadata": {
        "id": "Y62W2RsX5tcN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conclusion**"
      ],
      "metadata": {
        "id": "PHEadCbD5xHn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Random Forest is undoubtedly the best Ensemble method to use for our dataset with 70% accuracy!\n",
        "\n",
        "It is closely followed by the Bagging Classifier and the Histogram-Based Gradient Boosting which both give us 67% accuracy.\n",
        "\n",
        "Here is what you should remember about the 3 types of Ensemble algorithms:\n",
        "\n",
        "<table>\n",
        "<tr><th></th><th>Learning</th><th>Technique</th><th>Advantage</th></tr>\n",
        "<tr><td><b>Bagging</b></td><td>Parallel</td><td>Assembly of different models</td><td>Diversified approach</td></tr>\n",
        "<tr><td><b>Boosting</b></td><td>Sequential</td><td>Assembly of several versions of the same model</td><td>Successive Improvement</td></tr>\n",
        "<tr><td><b>Stacking</b></td><td>Parallel + Stacked</td><td>Assembly of different models evaluated by a final model</td><td>Assembly evaluated by an Independent model</td></tr>\n",
        "</table>\n",
        "\n",
        "This is the end of this extensive article on Ensemble methods.\n",
        "\n",
        "**If you’ve made it this far, you’ve accumulated a lot of information.**\n",
        "\n",
        "**In the future, you can refer to the schemes that provide a solid foundation for understanding Ensemble methods.**\n",
        "\n",
        "**Use them!**\n",
        "\n",
        "There is a reason why they’re the favorites in Machine Learning competitions.\n",
        "\n",
        "**With them, your models will reach the next level.**\n",
        "\n",
        "And if you are looking for other techniques to improve your Machine Learning algorithms,follow this way:\n",
        "\n",
        "- [Normalize data](https://inside-machinelearning.com/en/normalize-your-data/)\n",
        "- [Cross-Validation](https://inside-machinelearning.com/en/cross-validation-tutorial/)\n",
        "- [Changing the models hyperparameters](https://inside-machinelearning.com/en/decision-tree-and-hyperparameters/)\n",
        "- Data Augmentation\n",
        "\n",
        "See you soon on Inside Machine Learning 😉"
      ],
      "metadata": {
        "id": "0uBs1Fhc53Ff"
      }
    }
  ]
}